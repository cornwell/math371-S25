{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (9 points) Read the `Loan.csv` data set into Python. This data set is for binary classification, predicting a loan default, or no default. The labels the data set are contained in the column `default`.\n",
    "\n",
    "* **(a)** Write functions to carry out batch gradient descent, as described in class, for a logistic model on this data. You should include a function to compute the gradient of the loss, given the parameters. Another function, which uses the gradient function, should carry out the updates. A cutoff on the maximum number of iterations can be used, but there should also be a check for the stopping criterion discussed in class.\n",
    "* **(b)** Use the functions from (a) to train a logistic model. Use a learning rate of 0.7 and initial parameters that are all 0.\n",
    "* **(c)** If the output of the logistic model is 0.5 or larger, predict label 1, and label 0 otherwise. Determine the accuracy of your model on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (2 points) Suppose that we have sample data $\\mathcal S = \\{({\\bf x}_i, y_i)\\}$, $i=1,\\ldots,n$, with each ${\\bf x}_i \\in \\mathbb R^2$. As done in class notes, write the two coordinates of ${\\bf x}_i$ as $(x_{i,1}, x_{i,2})$.  Say a logistic model, but with quadratic terms is made &ndash; setting parameters $\\omega = (v_1, v_2, v_3, w_1, w_2, b)$, the parameterized function is: $$f_{\\omega}({\\bf x}_i) = \\sigma(v_1x_{i,1}^2 + v_2x_{i,1}x_{i,2} + v_3x_{i,2}^2 + w_1x_{i,1} + w_2x_{i,2} + b).$$\n",
    "With loss function $\\mathcal L_{\\mathcal S}$ (as in the slides), write out the partial derivatives $\\frac{d}{dv_i}\\mathcal L_{\\mathcal S}$, $i=1,2,3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (9 points) We'll show that the per-example loss, when working with a logistic model, is a convex function of $\\omega$. Recall that for $\\omega = ({\\bf w}, b)$ we set $f_{\\omega}({\\bf x}) = \\sigma({\\bf w}\\cdot{\\bf x}+b)$, where $\\sigma$ is the logistic function. We'll show that both \n",
    "    $$g_1(\\omega) = -\\log(f_{\\omega}({\\bf x})) \\quad\\text{and}\\quad  g_0(\\omega) = -\\log(1-f_{\\omega}({\\bf x})) $$\n",
    "    <span margin-left:4em>are convex functions.</span> A function $g:\\mathbb R^m\\to \\mathbb R$ is **convex** if and only if, for all $z_1, z_2\\in\\mathbb R^m$,\n",
    "    $$\\nabla g(z_1) \\cdot (z_2 - z_1) \\le g(z_2) - g(z_1).$$\n",
    "    <span margin-left:4em>In the case that</span> $m=1$, this says that $g'(z_1)\\cdot(z_2-z_1) \\le g(z_2)-g(z_1)$.\n",
    "\n",
    "> To show they are convex, fill in the following three steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(a)** Fixing a point ${\\bf x}_i$ from the sample data, first define $h(\\omega) = {\\bf w}\\cdot{\\bf x}_i + b$. Compute $\\nabla h$ (_note that partials are w.r.t. parameters_), and explain why $$\\nabla h(\\omega_1) \\cdot (\\omega_2 - \\omega_1) = h(\\omega_2) - h(\\omega_1)$$ \n",
    "for any pair $\\omega_1, \\omega_2$. And so, $h$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(b)** Second, let $F:\\mathbb R\\to\\mathbb R$ be (a twice differentiable function) such that $F''(z) > 0$ for all $z\\in\\mathbb R$. Show that $F$ is convex. \n",
    "\n",
    "&emsp;&emsp; (**Hint:** the assumption means that $F'(z)$ is an increasing function. Recall the _Mean Value Theorem_ from Calculus. Can you use it to get that $F'(z_1)\\cdot(z_2-z_1) \\le F(z_2)-F(z_1)$ for any $z_2>z_1$? What about when $z_2<z_1$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(c)** Finally, set $F_1(z) = -\\log(\\sigma(z))$ and $F_0(z) = -\\log(1-\\sigma(z))$, where $\\sigma$ is the logistic function. Check that $F_1''(z) > 0$ and $F_0''(z) > 0$ for all $z$.\n",
    "\n",
    "Now, $g_1(\\omega) = F_1(h(\\omega))$ and $g_0(\\omega) = F_0(h(\\omega))$. A composition of convex functions is a convex function (you don't need to show this), and so these functions are convex."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
