\frametitle{Implementing Gradient Descent}
    Some notes on implementation of the gradient descent updates.

    {\color{mygreen}1.} Each partial derivative of $\mathcal L_{\mathcal S}$, can compute it in one line of code. \pause Presuming \texttt{x} and \texttt{y} are arrays of coordinates for sample data, and that \lstinline[language=Python,basicstyle=\ttfamily,keywordstyle=\color{keywords}]{n = len(x)}, the following computes the partial derivatives:

\begin{codeblock}

\begin{python}
partial_m = (2/n)*np.sum( (m*x + b - y)*x )
partial_b = (2/n)*np.sum( (m*x + b - y) )
\end{python}

\end{codeblock}

    \pause
    {\color{mygreen}2.} To implement GD, want more than one function {--} at the least, one to compute the gradient (given current parameters); another that performs update and checks for stopping. \textit{Roughly}\ldots
\pause

\begin{pseudo}
## lr is learning rate; threshhold is for stopping;
input: X, y, lr, threshhold
p = initial array of parameters
while (max of last_update > threshhold){
    grad = compute_grad(p, X, y)
    last_update = | grad / p | ## entrywise array division
    # handle p[i] near 0
    p = p - lr*grad
}
return p
\end{pseudo}
