\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{Feb 25, 2025}
\title{Classification, Halfspaces, the Perceptron algorithm}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Classification tasks}

%%%%
\begin{frame}
\frametitle{Example of a classification task}
Use a model to predict if an image of a handwritten digit is 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. 

If ${\bf x}$ is the image (converted to a vector in some way), then your model's output $\hat{y}({\bf x})$, is the predicted digit. In the data you work with, you have an ``observation'' $y$ for which digit was, in fact, being written. 

\pause 
While $y$ and $\hat{y}$ are numbers, they are more like labels than something on the number line. Getting $\hat{y} = 4$, when $y = 5$, is not any better than getting $\hat{y} = 0$.

\centering
\includegraphics[height=0.25\textheight]{../../Images/50.png}
\end{frame}

%%%%
\begin{frame}
\frametitle{Close only counts in \ldots Regression}
When performing linear regression, on independent variables $x_0,x_1,\ldots,x_{d-1}$, had (affine) linear function $\hat{y} = p_0x_0+p_1x_{1}+\ldots+p_{d-1}x_{d-1} + p_d$; \newline 
values of function $\leftrightarrow$ prediction $\hat{y}$; error term $\varepsilon$, so that $y = \hat{y} + \varepsilon$.

\pause
In other words, observation $Y_{\bf x}$ for each data point ${\bf x} = (x_0,x_1,\ldots,x_{d-1}) \in \mathbb R^d$.  Have a linear ``model'' ${\bf x} \mapsto \hat{y}$ that approximates ${\bf x} \mapsto Y_{\bf x}$.\footnote{Should not think of this as (deterministic) function; rather, $Y_{\bf x}$ is random variable, e.g., simple linear regression: $Y_{x_0} = p_0x_0 + p_1 + \varepsilon$.} 

\pause
Would expect $|Y_{\bf x} - \hat{y}|$ to almost never be exactly 0; good model: one where $|Y_{\bf x} - \hat{y}|$ is small (but positive), on average. \newline 
\begin{center}``\emph{Regression}''\end{center}

\pause 
In a ``Classification'' task, the value $Y_{\bf x}$ is more like a \emph{label}. It might not even be a number and, if so, a $\hat{y}$ is just wrong or not; close doesn't count. That is, you want 

$\hat{y}$ to be the same as $Y_{\bf x}$, as much as possible with your model.
\end{frame}

%%%%
\begin{frame}
\frametitle{Half-space model}
    Assume data is from $\mathbb R^d$ for some $d>0$ and we only have two labels (e.g., this is Spam (\texttt{S}) or it is Not spam (\texttt{N})). 
    \pause

    A \emph{hyperplane} in $\mathbb R^d$ is an (affine) linear subspace that separates $\mathbb R^d$ in two. Perhaps we get lucky and can find a hyperplane $H$ so that data points with label \texttt{S} are on one side of $H$ and data with label \texttt{N} are on the other side.
    \pause
    Using coordinates $(x_1,x_2, \ldots, x_d)$ in $\mathbb R^d$, a hyperplane $H$ may be determined from $d+1$ numbers $w_1,w_2,\ldots,w_d$, and $b$. It consists of solutions to 
        \[w_1x_1 + w_2x_2\ldots + w_dx_d + b = 0.\]

    \begin{itemize}
        \item Rewriting in vector form: ${\bf w}=(w_1,w_2,\ldots,w_d)$, look for solutions ${\bf x}\in\mathbb R^d$ to the equation ${\bf w}\cdot{\bf x} + b = 0$.
        \item ${\bf w}$ is a vector that is orthogonal to a $(d-1)$-dimensional subspace of $\mathbb R^d$; $|b|$ corresponds to a translation away from the origin.
    \end{itemize}

\end{frame}

%%%%
\begin{frame}
\frametitle{Half-space model, continued}
    Using the notation from last slide: 
    
    a half-space model in $\mathbb R^d$ is determined by $d+1$ parameters $w_1,w_2,\ldots,w_d,b$; the first $d$ parameters grouped into a vector: ${\bf w} = (w_1,w_2,\ldots,w_d)$.
    
    \pause
    Given ${\bf x}\in \mathbb R^d$, the side of the hyperplane it is on is determined by the sign of ${\bf w}\cdot{\bf x} + b$.
    \begin{itemize}
        \item (Positive side) Say that $h({\bf x}) = 1$ if ${\bf w}\cdot{\bf x}+b > 0$.
        \item (Negative side) Say that $h({\bf x}) = -1$ if ${\bf x}\cdot{\bf x}+b < 0$. 
    \end{itemize}

    \pause
    If there exists a hyperplane, given by some ${\bf w}, b$, so that ${\bf x}$ has one of the labels if and only if it is on the positive side, the labeled data are called \textbf{linearly separable}. 

\end{frame}

%%%%
\begin{frame}
    \frametitle{Perceptron algorithm}
    Assuming that $({\bf x}_1, y_1), \ldots, ({\bf x}_n, y_n)$ is linearly separable, the Perceptron algorithm is a procedure that is guaranteed to find a hyperplane that separates the data.
    
    \pause
    
\end{frame}



\end{document}