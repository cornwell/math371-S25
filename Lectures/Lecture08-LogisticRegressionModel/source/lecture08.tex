\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{ulem}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={return, while, if, for, input},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1}
    {<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{Feb 27, 2025}
\title{Logistic Regression}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Reconsidering the Half-space Model}

%%%%
\begin{frame}
\frametitle{Decision boundaries}
Suppose function $h:\mathbb R^d\to \{y_1,y_2,\ldots,y_m\}$ gives predictions for a classification task (data points ${\bf x}\in\mathbb R^d$, with $m$ labels). \newline 
\pause
For $i\in\{1,2,\ldots,m\}$, define $C_{y_i}$ to be the set of points with label $y_i$, i.e.,
    \[C_{y_i} = h^{-1}(y_i) = \{{\bf x}\in\mathbb R^d\ |\ h({\bf x}) = y_i\}.\]

\pause
For $i \ne j$, a point is in \textbf{decision boundary for $h$} when it's in the boundary of both $C_{y_i}$ and $C_{y_j}$. For half-space model (last lecture) determined by ${\bf w}$ and $b$, decision boundary is the hyperplane {--} i.e., points ${\bf x}$ such that ${\bf w}\cdot{\bf x} + b = 0$.

\pause
Perceptron algorithm might give model $h$ with many data points that are \textit{near} the decision boundary.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[>=stealth, scale=0.4]
        \fill[blue!35!white, opacity = 0.2] (-4, 3) -- (5, 3) -- (5, -3) -- (2, -3)-- cycle;
        \fill[red!35!white, opacity = 0.2] (-4, 3) -- (-5, 3) -- (-5, -3) -- (2, -3)-- cycle;
        \draw[thick, ->] (-5.1, 0)-- (5.1, 0); 
        \draw[thick, ->] (0, -3.1) -- (0, 3.1); 
        \draw[very thick] (-4, 3) -- (2, -3); % node[above right]{\small $H = \{x + y + 1 = 0\}$}; 
        \fill[blue!80!black] (-2.9, 2.15) circle (2pt); 
        \fill[blue!80!black] (-2.49, 1.75) circle (2pt); 
        \fill[blue!80!black] (-1.93, 1.1) circle (2pt); 
        \fill[blue!80!black] (-0.35,-0.23) circle (2pt);
        \fill[blue!80!black] (0.08, -0.9) circle (2pt); 
        \fill[blue!80!black] (1.33, -2.07) circle (2pt);
        \fill[blue!80!black] (1.55, -2.37) circle (2pt); 
        \fill[blue!80!black] (2.23, -2.46) circle (2pt); 
        \fill[blue!80!black] (3.22, -0.37) circle (2pt); 
        \fill[blue!80!black] (2.72, 1.37) circle (2pt); 
        \fill[red!80!black] (-3.11, -1.97) circle (2pt);
        \fill[red!70!black] (-3, 0.5) circle (2pt); 
        \fill[red!70!black] (-3.5, -1) circle (2pt); 
        \fill[red!70!black] (-2, -1.5) circle (2pt); 
        \fill[red!70!black] (-1.87, -2.6) circle (2pt); 
    \end{tikzpicture}
    \caption{Many points near the decision boundary}
    \label{figure:close2decision}
\end{figure}

\end{frame}

%%%%
\begin{frame}
\frametitle{But\ldots data is messy}
\pause
Many points ``close'' to the decision boundary $\Rightarrow$ likely for newly observed data to appear on ``wrong side'' of decision boundary. Perceptron algorithm has nothing to correct for this.

\pause
Some thoughts:
\begin{itemize}
    \item When ${\bf x}$ close to decision boundary, we feel less \textit{confident} in giving the label $h({\bf x})$. In contrast, when ${\bf x}$ farther from boundary, and only one label is observed nearby, more confidence is warranted.
\pause
    \item Also, the immediate change of label across the boundary (a discontinuity in the model) \ldots perhaps not ``natural''?
\end{itemize}
\end{frame}

\section{Logistic model}

%%%%
\begin{frame}
\frametitle{Incorporating a probability into half-space model}
Don't only capture the sign of ${\bf w}\cdot{\bf x} + b$; instead, compose ${\bf w}\cdot{\bf x} + b$ with the \textbf{logistic function}.    
    \[\sigma(z) = \frac{1}{1+e^{-z}}.\]

\pause
\begin{itemize}
    \item $0 < \sigma(z) < 1$ for all $z\in\mathbb R$;
    \pause
    \item $\lim_{z\to\infty}\sigma(z) = 1$ and $\lim_{z\to-\infty}\sigma(z) = 0$;
    \item $\sigma(0) = 1/2$.
\end{itemize}
\pause 

\centering
\includegraphics[height=0.3\textheight]{../../Images/logistic_function_graph.png}

\end{frame}

%%%%
\begin{frame}
\frametitle{Logistic model, continued}
    ``Logistic regression'', used for binary classification, as follows.
    
    Find hyperplane $H$, as determined by some ${\bf w}$ and $b$, that fits labeled data well; given new ${\bf x}\in\mathbb R^d$, find $z = {\bf w}\cdot{\bf x}+b$, then compute $\sigma(z)$. 
    \pause
    \begin{itemize}
        \item (Logistic regression) $\sigma(z)$, the \textit{probability} that the label is $+1$.
    \end{itemize}
    
    \pause
    \begin{enumerate}
        \item If ${\bf w}\cdot{\bf x}+b > 0$ is very large (${\bf x}$ far away from $H$ and on positive side), then  $\sigma(z)$ is very close to $1$.
        \pause
        \item If ${\bf w}\cdot{\bf x}+b < 0$ has abs.\ value very large (${\bf x}$ far away from $H$ on negative side), then  $\sigma(z)$ very close to $0$.\footnote{Since $-1$ is only other label, high probability of having label $-1$.}
        \pause
        \item If ${\bf x}$ is contained in $H$ itself, $z=0$ and $\sigma(z) = 0.5$.
    \end{enumerate}
    \pause
    Logistic model (a binary classifier): $h({\bf x})=1$ if $\sigma({\bf w}\cdot{\bf x}+b) \ge 0.5$, and $h({\bf x})=-1$ otherwise. 
    \begin{itemize}
        \item \textit{Remember that probability (certainty) in the prediction}.
        \pause 
        \item \textit{Logistic model helps decision boundary not be near data} (more on this later).
    \end{itemize}

\end{frame}

%%%%
\begin{frame}
\frametitle{Logistic model, continued}
    ``Logistic regression'', used for binary classification, as follows.
    
    Find hyperplane $H$, as determined by some ${\bf w}$ and $b$, that fits labeled data well; given new ${\bf x}\in\mathbb R^d$, find $z = {\bf w}\cdot{\bf x}+b$, then compute $\sigma(z)$. 
    \begin{itemize}
        \item (Logistic regression) $\sigma(z)$, the \textit{probability} that the label is $+1$.
    \end{itemize}
    
    Logistic model (a binary classifier): $h({\bf x})=1$ if $\sigma({\bf w}\cdot{\bf x}+b) \ge 0.5$, and $h({\bf x})=-1$ otherwise. 
    
    \pause 
    Some rationale for the use of the logistic function $\frac{1}{1+e^{-z}}$ in this process, comes from an inverse direction. 
    \begin{itemize}
        \item In using \textit{Maximum Likelihood Estimation} to get binary classification model, some typical simplifying assumptions on conditional probability $P$, of label 1 given input ${\bf x_i}$ \qquad $\leadsto$ linear function for $\log\left(\frac{P}{1-P}\right)$. \newline 
        \item $\log\left(\frac{P}{1-P}\right)$ \textit{is called the } \textbf{log odds } \textit{or } \textbf{logit}.
    \end{itemize}

\end{frame}
%%%%
\begin{frame}
    \frametitle{How to find ${\bf w}$ and $b$}
    Given $\{\pm1\}$ labeled data, how do we go about finding ${\bf w}$ and $b$ to use in the logistic (regression) model?

    \pause
    \begin{itemize}
        \item Even if data is linearly separable, Perceptron algorithm does not try to make hyperplane be positioned ``away from'' data (Disadvantage). 
        \item If data is not linearly separable, what should be done?
    \end{itemize}

    \pause
    Future lectures: Will discuss using \emph{optimization} (calculus-based) to find best parameters $w_1,w_2,\ldots,w_d,b$; process called Gradient Descent. 
    \pause
    \begin{itemize}
        \item Relevant: relationship between {gradient} of a function and its {directional derivative} (discussed in Calc III).
    \end{itemize}
\end{frame}
%%%%
\begin{frame}
    \frametitle{More messy versus less messy data}
    Could introduce additional parameter (more flexibility).

    For $k > 0$, define \[\sigma_k(z) = \frac{1}{1+e^{-kz}}.\]

    \pause
    $0 < k < 1$: values of $\sigma_k(z)$ transition from $0$ to $1$ more slowly.\newline 
    $k > 1$: values of $\sigma_k(z)$ transition from $0$ to $1$ more quickly. (think about derivative)

    \pause
    Hence, if know data has more noise, might use $0 < k < 1$ to decrease measure of confidence in prediction. In contrast, very ``clean'' data, interpretation of the model might benefit from $k>1$.\footnote{I first saw the idea to introduce $k$, and interpret the model this way, from Eli Grigsby at Boston College.}
    
    \pause
    (\textbf{Left:} graph with $k=5$; \textbf{Right:} applied to points in $\mathbb R^2$)
    \centering
    \begin{multicols}{2}
    \includegraphics[height=0.25\textheight]{../../Images/logistic_function_kis5.png}

    \includegraphics[height=0.25\textheight]{../../Images/logistic_function_appliedtohalfspace.png}
    \end{multicols}
    
\end{frame}

\end{document}