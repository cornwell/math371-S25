\documentclass[smaller]{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{etoolbox}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{ulem}
\usepackage{fontspec}
%\usepackage[T1]{fontenc}
%\setmainfont{Cambria}
%\usefonttheme{serif}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}



\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={return, while, if, for, input},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1}
    {<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\newcommand{\x}{\textbf{x}}
\newcommand{\ix}[1]{{\it #1}}

\author{Chris Cornwell}
\date{April 14, 2025}
\title{Selecting the Machine Learning Model}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
    \frametitle{Outline}
    \tableofcontents
\end{frame}

\section{Overfitting \\ Having only concern be to minimize empirical loss}

%%%%
\begin{frame}
\frametitle{Intro}
Given sample data $\mathcal S$, consisting of greyscale images of handwritten digits $0,1,\ldots,9$, say we want a decision tree model to predict the correct digit, given an image. 

Let the images be $p$ pixels by $p$ pixels. For an image, we might take the $p^2$ greyscale values (0{--}255) and lay them end to end to get a vector in $\mathbb R^{p^2}$. Then, could train a decision tree with however many splits are needed so that each leaf has points in $\mathcal S$ with only one label. 
\begin{itemize}
    \item Gives 100\% accuracy on $\mathcal S$, the data used to determine the parameters.
    \item But, the model won't perform nearly as well on data not from $\mathcal S$. 
\end{itemize}

Let's look at doing exactly this, using images of digits available from the Python package scikit-learn. 
\end{frame}

%%%%
\begin{frame}[fragile]
    \frametitle{Decision tree model for handwritten digits}
The images of digits from scikit-learn are $8$ pixels by $8$ pixels. With the submodule \ttt{datasets} imported from \ttt{sklearn}, the data can be loaded as follows.

\begin{codeblock}

\begin{python}
data = datasets.load_digits()
x = data.images
y = data.target
\end{python}

\end{codeblock}

To convert each $8\times 8$ array to a vector in $\mathbb R^{64}$, can use the \ttt{reshape} method; after assigning $\texttt{x}$ and $\texttt{y}$ as above, code below would output \ttt{(1797, 64)}.

\begin{codeblock}

\begin{python}
# make each 8x8 array into 1d array with 64 entries
x = x.reshape(len(x), -1)
x.shape
\end{python}
    
\end{codeblock}

Randomly select 20\% of the data to test model with {--} it won't be used to determine the decision tree.

\begin{codeblock}

\begin{python}
n_test = int(0.2*len(x))
test_indices = np.random.choice(len(x), size=n_test, replace=False)
train_indices = np.delete(np.arange(len(x)), test_indices)
x_test, y_test = x[test_indices], y[test_indices]
x_train, y_train = x[train_indices], y[train_indices]
\end{python}
    
\end{codeblock}
    
*\textit{Alternatively, you can use the function} \lstinline[language=Python,stringstyle=\ttfamily]{train_test_split} \textit{from the scikit-learn submodule} \lstinline[language=Python,stringstyle=\ttfamily]{model_selection}.
\end{frame}

%%%%
\begin{frame}[fragile]
    \frametitle{Decision tree model for handwritten digits}
With the setup from the last slide, use the training data to determine the decision tree.

There is a submodule \lstinline[language=Python,stringstyle=\ttfamily]{tree} within scikit-learn that we can use.

\begin{codeblock}

\begin{python}
model = tree.DecisionTreeClassifier()
model.fit(x_train, y_train)
\end{python}

\end{codeblock}

The default behavior of the \ttt{.fit()} method is that the tree has as many splits (decision stumps) as needed so that each leaf consists of points with a single label. The accuracy on \lstinline[language=Python,stringstyle=\ttfamily]{x_train} is, therefore, 100\%. 

The accuracy of the model on \lstinline[language=Python,stringstyle=\ttfamily]{x_test} will depend some on which points were put into \lstinline[language=Python,stringstyle=\ttfamily]{x_test}; however, it tends to be around only 85\%.

What happened is that the \textit{hypothesis class}, that set of functions that were possible outcomes to be the trained model, was too large (allowed too many possibilities for the resulting model). 
\end{frame}

%%%%
\begin{frame}
    \frametitle{Keeping the hypothesis class small}
We discuss a (somewhat absurd) example to demonstrate the point that 
\begin{center} small empirical loss $\not\Rightarrow$ small population loss\end{center}

\begin{wrapfigure}{r}{0.4\textwidth}
    \begin{center}
        \includegraphics[width=0.38\textwidth]{../../Images/fourth_square-13v37.png}
    \end{center}
    \caption{Upper-left square labeled -1}
\end{wrapfigure}

Allowing for \textit{any} function as a possible model, for any training data $\mathcal S = \{(\x_i, \ix y_i)_{i=1}^n$, with labels in $\{1,-1\}$, we set 
        \[f_S(\x) = \begin{cases}\ix y_i,& \textrm{ if }\x = \x_i \\ 
                                    -1,& \textrm{ otherwise.}
        \end{cases}\]
Regardless of $\mathcal S$, or the population distribution that it is a sample from, the empirical loss $\mathcal L_{S}(f_S) = \frac{\#\{i:\ \ix y_i \ne f_S(\x_i)\}}{n}$ is zero.
\end{frame}


%%%%
\begin{frame}
    \frametitle{Keeping the hypothesis class small}
We discuss a (somewhat absurd) example to demonstrate the point that 
\begin{center} small empirical loss $\not\Rightarrow$ small population loss\end{center}

\begin{wrapfigure}{r}{0.4\textwidth}
    \begin{center}
        \includegraphics[width=0.38\textwidth]{../../Images/fourth_square-13v37.png}
    \end{center}
    \caption{Upper-left square labeled -1}
\end{wrapfigure}

Suppose that the population data consists of points in $[0,1]^2$ (the square of points with both coordinates between $0$ and $1$), and that a point $(x_1,x_2)$ has label $-1$ if and only if $0\le x_1 < 0.5$ and $0.5< x_2 \le 1$ (See Figure).
\vspace*{12pt}

Since a sample $\mathcal S$ is finite, a randomly chosen point has probability $0$ of being in $\mathcal S$. Thus, given a random $\x\in[0,1]^2$, with probability $3/4$ the predicted label $f_S(\x)$ is wrong.

The issue with both this model, and the decision tree that had no restriction on depth, is that the \textbf{variance} \textit{of the hypothesis class} is too large.
\end{frame}

%%%%
\begin{frame}
    \frametitle{Bias-Variance Trade-off}
    Suppose that we have chosen a hypothesis class (a class of parameterized functions, say), and a procedure that, given a training sample $\mathcal S$ from the population, determines a prediction function $f_S$.

    In order to understand the \textit{expected} population loss, we want to consider the variance of $f_S(\x)$, not only over the input space but over possible training sets $\mathcal S$. That is, over all $(\x, y)$ and $S$, the expected value $\mathbb E[(f_S(\x) - y)^2]$.

    Let $\overline{h}$ be the expected prediction function, over training sets.\footnote{This takes some advanced mathematics to carefully describe. It means there is a ``measure'' on sets of functions, which allows you to integrate over a set of functions to get a new function.} In practice, you could approximate $\overline{h}$ in the following way: for samples $S_1,\ldots,S_N$, each with the same number of points, drawn i.i.d.\ from the population, then for sufficiently large $N$, 
        \[\overline{h}(\x) \approx \frac{1}{N}\sum_{i=1}^Nh_{S_i}(\x).\]
\end{frame}


% Now, to define \( IG\) of a split, first let \( r = {\bf P}(y=1)\), the probability that the label is 1 (in the particular subset you have restricted to <em>before</em> the split). Let \( m\) be the number of points under consideration. In our example above, and <em>after</em> the first split along \( x_2 = -1.8\) was made, say we are in the positive side where there is 1 red point (negatively labeled), and are trying to figure out where to make the second splitting. Then \( m = 38\) (there are \( 38\) points above the horizontal line), and \(r = 37/38\).

% Next, for a potential splitting, let \(r_+ = {\bf P}(y=1\ |\ x\text{ on the + side})\) be the fraction of blue (+1 labeled) points on the positive side of that split. Similarly, define \(r_- = {\bf P}(y=1\ |\ x\text{ on the - side})\). Also, let \( m_{\pm}\) be the total number of points on each side. In the continuing example, say the potential split is along the line \( x_1 =-3.9\) (one of the second level splits depicted in the figure above). Then \(r_+ = 35/35 = 1\) and \(r_- = 2/3\).

% Now, with \(r, r_+,\) and \(r_-\) defined, the information gain of a split \( s\) is \( IG(s) = e(r) - (\frac{m_+}{m}e(r_+) + \frac{m_-}{m}e(r_-))\). Note that it actually doesn't matter which side of the line is the positive or negative side, just where the splitting line is. For the continuing example, the information gain is
% <p style="text-align: center;">\( e(37/38) - (\frac{35}{38}e(1) + \frac{3}{38}e(2/3)) \approx 0.07.\)</p>
% If you were to use \( IG\) to build a decision tree, then at each step you must find the split, on the subset of the data under consideration, that has maximal information gain.

% <sup id="footnote4">4.</sup> <span style="color: gray;">It is important to consider this a definition, not a consequence of the logarithmic expression given above. While you can get \( 0\) from <em>a limit</em> of that expression as \( r\to 0^+\), it is <span style="text-decoration: underline;">not</span> equal to its value. Trying to say it is might not lead to trouble if you only think this way for the purpose of the calculation of a single entropy function value. However, if you attempt to do any arithmetic and use properties of logarithms, thinking of \( 0\log 0\) as being equal to \( 0\) will easily lead to statements like \( 1 = 2\).</span>

% <sup id="footnote5">5.</sup> <span style="color: gray;">The logarithm function can have any base. Most of the time, in discussions of Information Gain, the base is 2 due to the area of study that the idea comes from.</span>

\section{Clustering}

\begin{frame}
    \frametitle{What is clustering?}
    Intuitively, want to group together data points into subsets, i.e., \textit{clusters}, so that \textit{similar} points are in the same cluster and dissimilar points are in different clusters. These data do not have labels. 
    
    \begin{itemize}
        \item Presuming the numerical coordinates of the data have meaning, ``similar'' points could be those that are sufficiently close.
    \end{itemize}
    
    \textbf{Goal:} Given sample data $\mathcal S=\{\x_i\}_{i=1}^n$, want to determine clusters $C_1,C_2,\ldots,C_k$ (for some $k$), so that $C_1\cup C_2\cup \ldots\cup C_K= \mathcal S$ and $C_i\cap C_j = \emptyset$ when $i\ne j$.
    \vfill 

    \textit{An inherent difficulty}: the clustering problem is ill-defined. 
    \begin{itemize}
        \item For example, you might have a sequence of data points $\x_1,\ldots,\x_m$ such that $\x_i$ and $\x_{i+1}$ are similar (close enough to be called similar) for each $1\le i\le m-1$, however $\x_1$ and $\x_m$ might be very dissimilar. While $\x_i$ and $x_{i+1}$ should be in the same cluster, this causes the problem of putting $\x_1$ and $\x_m$ ending up in the same cluster.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Applications of clustering}
    Despite its ill-defined nature, clustering is needed (sometimes in an initial processing step) for many data analysis tasks. 

    Some examples include:
    \begin{itemize}
        \item Market segmentation: clustering expected customers based on characteristics, either for targeted marketing or to inform product design.
        \item Gene expression clustering: computational biologists will cluster genes based on how their expression in different experiments.
        \item Astronomical data analysis: clustering stars or galaxies based on their proximity.
        \item Social network analysis: for example, identifying ``communities'' (clusters of users) based on interactions on social media.
    \end{itemize}
\end{frame}

%%%%
\begin{frame}
    \frametitle{$k$-means}
    Let $\mathcal S=\{\x_i\}_{i=1}^n$, with each $\x_i\in\mathbb R^d$ for some dimension $d$.

    \begin{itemize}
        \item The $k$-means algorithm is a common approach for clustering $\mathcal S$. 
        \item You choose $k$, and your goal is to find $k$ clusters, $C_1, C_2,\ldots, C_k$ so that $\mathcal S=C_1\cup\ldots C_k$, where we want the sum $\sum_{j=1}^k\sum_{\x_i\in C_j} |\x_i - \mu_{j}|^2$ to be minimized. 
        \item Here, $\mu_j$ is the centroid of $C_j$, meaning that $\mu_j$ is the point in $\mathbb R^d$ that minimizes, over $\mu\in\mathbb R^d$, the sum $\sum_{\x\in C_j}|\x - \mu|^2$.
        \begin{itemize}
            \item Note that, in fact, $\mu_j = \frac{1}{|C_j|}\sum_{\x \in C_j}\x$.
        \end{itemize}
    \end{itemize}

    \centering
    \includegraphics[height=0.35\textheight]{../../Images/kmeans_0.png}
\end{frame}

%%%%
\begin{frame}
    \frametitle{$k$-means Procedure}
    
    The procedure to carry out $k$-means clustering is the following. First, randomly initialize $k$ centroids. Then: 
    \begin{enumerate}
        \item For each $i=1,2,\ldots,n$, determine $1\le j(i)\le k$, which is the index such that $\mu_{j(i)}$ is the closest centroid to $\x_i$. A point $\x_i$ is in $C_j$ when $j=j(i)$.
        \item Update $\mu_1,\mu_2,\ldots,\mu_k$ so that, for $1\le j\le k$, the vector $\mu_j$ is the centroid of the points in $C_j$.
        \item Iterate steps 1 and 2 until there is no $\x_i$ that ``changes its cluster'', i.e., until the assignment $i\mapsto j(i)$ that is made in 1 is the same as it was in the previous iteration.
    \end{enumerate}

    At each iteration, if some point has changed the cluster it is in, then the value of $\sum_{j=1}^k\sum_{\x_i\in C_j} |\x_i - \mu_{j}|^2 = \sum_{i=1}^n|\x_i - \mu_{j(i)}|^2$ decreases. Hence, the algorithm terminates because, as there are finitely many points in $\mathcal S$, there are only a finite number of possibilities for the list $\mu_1, \mu_2, \ldots, \mu_k$.
\end{frame}

%%%%
\begin{frame}
    \frametitle{Visual of $k$-means in $\mathbb R^2$}
    
    \begin{wrapfigure}{r}{0.3\textwidth}
        \begin{center}
            \includegraphics[width=0.25\textwidth]{../../Images/kmeans_0.png}
        \end{center}
        \caption{Initial centroids}
    \end{wrapfigure}

    We show a visualization of the procedure for $k$-means on data in $\mathbb R^2$, with $k=2$.
    \vspace*{12pt}

    Initially, we choose two arbitrary centroids. These are drawn in black.

    Then, the first step is to go through all of our data (drawn in ) and determine which of the two centroids is closest to each point. This assigns each point to one of the two clusters.

\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{Visual of $k$-means in $\mathbb R^2$}
    
    \begin{wrapfigure}{r}{0.3\textwidth}
        \begin{center}
            \includegraphics[width=0.25\textwidth]{../../Images/kmeans_1.png}
        \end{center}
        \caption{Assigned clusters}
    \end{wrapfigure}

    We show a visualization of the procedure for $k$-means on data in $\mathbb R^2$, with $k=2$.
    \vspace*{12pt}

    Here, the two clusters are shown. 
    
    The points in one are drawn in light blue and the points in the other are drawn in red.  

    The centroid of each cluster is displayed, outlined in the color of its cluster. The next step is to recompute the centroids.

\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{Visual of $k$-means in $\mathbb R^2$}
    
    \begin{wrapfigure}{r}{0.3\textwidth}
        \begin{center}
            \includegraphics[width=0.25\textwidth]{../../Images/kmeans_2.png}
        \end{center}
        \caption{Updated clusters}
    \end{wrapfigure}

    We show a visualization of the procedure for $k$-means on data in $\mathbb R^2$, with $k=2$.
    \vspace*{12pt}

    The two newly computed centroids are shown. 
    
    In addition, we reassign points based on the (new) centroid that is closest. 

    We notice that 4 of points have been removed from the blue cluster into the red cluster; also, 3 points have been removed from red cluster into the blue.

\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{Visual of $k$-means in $\mathbb R^2$}
    
    \begin{wrapfigure}{r}{0.3\textwidth}
        \begin{center}
            \includegraphics[width=0.25\textwidth]{../../Images/kmeans_3.png}
        \end{center}
        \caption{Updated clusters}
    \end{wrapfigure}

    We show a visualization of the procedure for $k$-means on data in $\mathbb R^2$, with $k=2$.
    \vspace*{12pt}

    Again, we compute new centroids and reassign points based on the centroid that is closest. 
    
    In this step, there is 1 point that was previously in the blue cluster which changes into red. And 4 points that were in the red cluster have been moved into the blue cluster.

\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{Visual of $k$-means in $\mathbb R^2$}
    
    \begin{wrapfigure}{r}{0.3\textwidth}
        \begin{center}
            \includegraphics[width=0.25\textwidth]{../../Images/kmeans_4.png}
        \end{center}
        \caption{Updated centroids}
    \end{wrapfigure}

    We show a visualization of the procedure for $k$-means on data in $\mathbb R^2$, with $k=2$.
    \vspace*{12pt}

    We compute new centroids again. 
    
    This time there is no change in the way that points are assigned to clusters when finding the closest centroid. 
    
    Since there is no change to the clustering assignment, the procedure terminates with these centroids.

\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{$k$-means depends on Initial Centroids}
    
    \begin{figure}
        \begin{center}
            \includegraphics[height=0.5\textheight]{../../Images/kmeans_balldata0.png}
        \end{center}
        \caption{A data set to cluster}
    \end{figure}
\phantom{With initial centroids of $(0,0.5)$ and $(0, 0.25)$}
\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{$k$-means depends on Initial Centroids}
    
    \begin{figure}
        \begin{center}
            \includegraphics[height=0.5\textheight]{../../Images/kmeans_balldata1.png}
        \end{center}
        \caption{A data set to cluster}
    \end{figure}
With initial centroids of $(0,0.5)$ and $(0, 0.25)$.
\vfill
\end{frame}

%%%%
\begin{frame}
    \frametitle{$k$-means depends on Initial Centroids}
    
    \begin{figure}
        \begin{center}
            \includegraphics[height=0.5\textheight]{../../Images/kmeans_balldata2.png}
        \end{center}
        \caption{A data set to cluster}
    \end{figure}
With initial centroids of $(0,0.26)$ and $(0, 0.1)$.
\vfill
\end{frame}

\begin{frame}
    \frametitle{Density-Based Spatial Clustering for Applications with Noise}
    DBSCAN clustering does not require choosing a number of clusters at the start. 

    \begin{itemize}
        \item Near a given point in $\mathcal S$, whether it gets included in a nearby cluster is based on the \textit{density} of points (in $\mathcal S$) near that given point. 
        \item The procedure builds a directed graph (network of vertices and edges); vertices are points in $\mathcal S$; which edges are included is based on a choice of parameters, and takes into account the nearby density of points. 
        \item Connectedness through edges of the graph determines when points are in the same cluster.
    \end{itemize}
    An example of a graph built in the DBSCAN procedure is drawn below. Points are colored according to their cluster.

    \centering
    \includegraphics[height=0.35\textheight]{../../Images/dbscan_graph.png}
\end{frame}

\end{document}