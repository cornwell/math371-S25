\documentclass[smaller]{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{etoolbox}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{ulem}
\usepackage{fontspec}
%\usepackage[T1]{fontenc}
%\setmainfont{Cambria}
%\usefonttheme{serif}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}



\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={return, while, if, for, input},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1}
    {<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\newcommand{\x}{\textbf{x}}
\newcommand{\ix}[1]{{\it #1}}

\author{Chris Cornwell}
\date{April 1, 2025}
\title{A survey of some Machine Learning models}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\section{Support Vector Machines}

%%%%
\begin{frame}
\frametitle{Setup}
    Similar to a logistic model, a \textbf{support vector machine} is a model for binary classification, using a hyperplane, of the form $\{\x\in\mathbb R^d\ | \textbf{w}\cdot\x+b = 0\}$, as decision boundary.

    However, the optimization goal is different. 
    
    \pause
    Given sample data $\mathcal S = \{(\x_i, \ix y_i)\}_{i=1}^n$, the goal is to minimize the value of $\frac12|\textbf{w}|^2$, the vector norm\footnote{The multiple $\frac12$ is non-essential here, but the convention is to include it.}, subject to the condition that for all $1\le i\le n$, $\ix y_i(\textbf{w}\cdot{\x}_i + b) \ge 1$ is satisfied.\footnote{Recall, we pointed out how this is possible when the data is linearly separable.}

    To work with a data set that is not linearly separable, one introduces so-called ``slack variables'' $\xi_i \ge 0$, $i=1,\ldots,n$ into the inequalities. They change to $\ix y_i(\textbf{w}\cdot\x_i + b) \ge 1 - \xi_i$.

    The reason for wanting to minimize $\frac12|\textbf{w}|^2$?
    \begin{itemize}
        \item Supposing no $\x_i$ passes through hyperplane with parameters $\textbf{w}, b$, we can scale both the normal vector and $b$ so that $\min_i|\textbf{w}\cdot\x_i + b| = 1$. 
        \item The distance from any $\x\in\mathbb R^d$ to the hyperplane is $\frac{|\textbf{w}\cdot\x + b|}{|\textbf{w}|}$. So, if $\x_i\in\mathcal S$ is such that $|\textbf{w}\cdot\x_i + b| = 1$, then its distance to decision boundary is $\rho = \frac1{|\textbf{w}|}$.
        \item Want to maximize distance to decision boundary, so want to minimize $|\textbf{w}|$.
    \end{itemize}
\vspace*{12pt}

\end{frame}

%%%%
\begin{frame}
\frametitle{Constrained minimization and SVM}
We can understand minimizing $\frac12|\textbf{w}|^2$ subject to $\ix y_i(\textbf{w}\cdot{\x}_i + b) \ge 1$ with a Lagrangian. (Method of Lagrange multipliers; see Section 7.2 in the Mathematics for Machine Learning book.)

For $\emph{\alpha}=(\alpha_1,\ldots,\alpha_n)$, with $\alpha_i\in\mathbb R$, Lagrangian is 
    \[L(\textbf{w}, b, \emph{\alpha}) = \frac12|\textbf{w}|^2 - \sum_{i=1}^n\alpha_i\left(\ix y_i(\textbf{w}\cdot\x_i + b) - 1\right).\]
It is minimized when 
    \begin{align*}
        \nabla_{\textbf{w}}L = 0  &\quad\Rightarrow\quad \textbf{w} = \sum_{i=1}^n\alpha_i\ix y_i\x_i; \\
        \nabla_{b}L = 0  &\quad\Rightarrow\quad \sum_{i=1}^n\alpha_i\ix y_i = 0; \\ 
        \alpha_i\left(\ix y_i(\textbf{w}\cdot\x_i + b)-1\right) = 0 &\quad\Rightarrow\quad 
         \alpha_i = 0 \quad\text{ OR }\quad \ix y_i(\textbf{w}\cdot\x_i + b) = 1.
    \end{align*}
\textbf{Support vectors} are those $\x_i$ for which $\alpha_i\ne0$, and so $\textbf{w}\cdot\x_i + b = \pm 1$.
\end{frame}

%%%%
\begin{frame}
\frametitle{Lagrangian Duality}
Something interesting happens when we convert the previous Lagrangian optimization problem into its ``Lagrangian dual problem.'' This means that we take the minimum solution for $\textbf{w}$, put it into $L(\textbf{w},b,\emph{\alpha})$ and want multipliers $\alpha_i\ge 0$ that \textit{maximize} the value of this. That is, maximize 
    \[\frac12\left|\sum_{i=1}^n\alpha_i\ix y_i\x_i\right|^2 - \sum_{i=1}^n\alpha_i\left(\ix y_i\left(\sum_{j=1}^n\alpha_j\ix y_j\x_j\right)\cdot\x_i + \ix y_ib - 1\right).\]
Rearranged, you can rewrite it:

    \[\max_{\emph{\alpha}} \sum_{i=1}^n\alpha_i - \frac12\sum_{i,j=1}^n\alpha_i\alpha_j\ix y_i\ix y_j \x_i\cdot\x_j\]
subject to $\alpha_i\ge 0$ and $\sum_{i=1}^n\alpha_i\ix y_i = 0$.

This optimization problem only depends on knowing $\x_i\cdot\x_j$ for each $(i,j)$, and this leads to what are called \textbf{kernel methods} that are very computationally efficient and allow one to use SVM models that have non-linear decision boundaries. 
\end{frame}

%%%%
\begin{frame}
    \frametitle{SVMs via Gradient Descent}
    An alternative for optimizing an SVM classifier is to do so with a loss function. The loss function has a fair amount of similarity to the log-loss function we used in logistic regression; however, the per-example losses use a piecewise linear function.

    When $\ix y_i = 1$ then, writing $z_i = \textbf{w}\cdot\x_i + b$, the per-example loss is $C\max(1-z_i, 0)$ for some constant $C$. Call this $C\text{cost}_1(z_i)$. When $\ix y_i=-1$ (and so $\tilde{\ix y}_i = 0$) then the per-example loss is $C\text{cost}_0(z_i) = C\max(1+z_i, 0)$. However, we also include the norm of $\textbf{w}$ in the loss: 

    \[\mathcal L_{\mathcal S}(\textbf{w}, b) = \frac12|\textbf{w}|^2 + \frac{1}{n}\sum_{i=1}^nC\left(\tilde{\ix y}_i\text{cost}_1(\textbf{w}\cdot\x_i + b) + (1 - \tilde{\ix y}_i)\text{cost}_0(\textbf{w}\cdot\x_i + b)\right).\]
\end{frame}

%%%%
\begin{frame}
    \frametitle{Similarity to Perceptron algorithm updates}
    Consider the \textbf{per-example loss}: given one $({\bf x}_i, y_i) \in\mathcal S$, have the term $-\tilde y_i\log(f_{\omega}({\bf x}_i)) - (1 - \tilde y_i)\log(1 - f_{\omega}({\bf x}))$.

    Its partial w.r.t.\ $w_j$ is 
        \[-\tilde y_ix_{i,j}\ast(1-f_{\omega}({\bf x}_i)) + (1-\tilde y_i)x_{i,j}f_{\omega}({\bf x}_i).\]

    \pause
    If $\tilde y_i=1$, but $f_{\omega}({\bf x}_i)$ is close to 0, then this is \textit{almost} $-x_{i,j} = -y_i x_{i,j}$. Were we to do an update with $\eta=1$ then we would have $w_j^{(t)} \approx w_j^{(t-1)} + y_i x_{i,j}$. (The Perceptron update in the $j^{th}$ coordinate.)

    \pause
    Likewise, if $\tilde y_i=0$, but $f_{\omega}({\bf x}_i)$ is close to $1$, then the partial is approximately $x_{i,j}$; so, using $\eta=1$, we would get $w_j^{(t)} \approx w_j^{(t-1)} - x_{i,j} = w_j^{(t-1)} + y_ix_{i,j}$. (Like the Perceptron algorithm update again.)
\end{frame}

%%%%
\begin{frame}
    \frametitle{Example with Logistic model}
    Given $\pm1$-labeled sample data $\mathcal S= \{({\bf x}_i, y_i)\}_{i=1}^n$, we found an expression for each partial derivative of $\mathcal L_{\mathcal S}$. In fact, if we call $w_{d+1} = b$ and $x_{i,d+1} = 1$ for every $1\le i\le n$, then 
        {\small 
        \[\frac{\partial}{\partial w_{j}}\mathcal L_{\mathcal S} = \frac1n\sum_{i=1}^n-\tilde y_ix_{i,j}(1-f_{\omega}({\bf x}_i)) + (1-\tilde y_i)x_{i,j}f_{\omega}({\bf x}_i)\]
        }
    for every $1\le j\le d+1$.

    \pause
    \textbf{Example:} consider simulated sample data below, drawn uniformly from a subset of $[-1,1]^2$, consisting of points at least a fixed distance from $H = \{(x_1,x_2)\in\mathbb R^2\ |\ 2x_1 - \frac23x_2 - \frac15=0\}$. Positively labeled points are in blue. 
    \vspace*{11pt}

    \centering
    \includegraphics[height=0.3\textheight]{../../Images/simulated-logregression-data.png}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Example with Logistic model}
    Given $\pm1$-labeled sample data $\mathcal S= \{({\bf x}_i, y_i)\}_{i=1}^n$, we found an expression for each partial derivative of $\mathcal L_{\mathcal S}$. In fact, if we call $w_{d+1} = b$ and $x_{i,d+1} = 1$ for every $1\le i\le n$, then 
        {\small 
        \[\frac{\partial}{\partial w_{j}}\mathcal L_{\mathcal S} = \frac1n\sum_{i=1}^n-\tilde y_ix_{i,j}(1-f_{\omega}({\bf x}_i)) + (1-\tilde y_i)x_{i,j}f_{\omega}({\bf x}_i)\]
        }
    for every $1\le j\le d+1$.

    \textbf{Example (cont'd):} Batch gradient descent, with learning rate $0.5$, stopping with threshhold $0.0005$, gives (approximately) parameters for: 
        \[\hat{H} = \{(x_1,x_2)\in\mathbb R^2\ |\ 2.01x_1 - 0.63x_2-0.26=0\}.\]
    \centering
    \includegraphics[height=0.3\textheight]{../../Images/Hfit-simulated-logregression-data.png}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Logistic regression on the Iris data}
    Recall the Iris data set: 150 points, 50 from each of three species of Iris flower. Two of the species in the data set, Iris versicolor and Iris virginica, are not linearly separable.

    \pause
    We can use gradient descent on the logistic model to find a hyperplane that does \textit{well} in classifying versicolor vs.\ virginica species.

    \pause
    For visualization, I first show just the first and fourth coordinates, and the results for logistic model in 2D.
    
    \vfill

    \centering
    \includegraphics[height=0.35\textheight]{../../Images/HalfSp_2DIrisData.png}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Logistic regression on the Iris data}
    Pictured below are selected lines found during the batch gradient descent. 
    
    \onslide<2->{
    As before, yellow-to-purple is progression through the procedure. Consecutive lines that are shown have $200$ updates between them; $\approx 4000$ updates in total.
    }

    \onslide<3->{
    The accuracy in this 2D projection is 92\% (the final hyperplane correctly labeled 92 out of 100).\footnote{Recall, the model labels the point with $+1$ when $f_{\omega}({\bf x}) \ge 0.5$.}
    }

    \onslide<4->{
    The model on the points in $\mathbb R^4$ took less updates (just under $3000$). It had 97\% accuracy on the data.
    }

    \vfill

    \centering
    \onslide<1->{
    \includegraphics[height=0.35\textheight]{../../Images/GDon2DIrisData.png}
    }
\end{frame}

\end{document}