\frametitle{SGD Procedure for SVM with Kernels (and slack variables)}
    Here is the SGD procedure with a kernel (using the Gram matrix $[K(\x_i, \x_j)]$).

    \pause
    The algorithm uses gradient descent to iteratively find $\alpha_1^{(t)},\ldots,\alpha_n^{(t)}$ at step $t$. During it, scalars $\beta_1^{(t)},\ldots,\beta_n^{(t)}$ are used. Writing \lstinline[language=Pseudo,basicstyle=\ttfamily]{beta[i][t]} for $\beta_i^{(t)}$ and \lstinline[language=Pseudo,basicstyle=\ttfamily]{psi} for $\psi$, the relation to our previous SGD algorithm is \lstinline[language=Pseudo,basicstyle=\ttfamily]{theta[t] = sum( beta[i][t]*psi(X[i]), i=1,...,n )}.
\pause

\begin{pseudo}
## lambda: the coeff of regularization; T: the number of iterations
## K: n by n Gram matrix
input: K, y, lambda, T
beta[i][1] = initial zero for i from 1,...,n
for (t == 1,...,T){
    alpha[i][t] = beta[i][t]/(2*lambda*t)  # for i from i,...,n
    Choose j uniformly at random from 1,...,n
    beta[i][t+1] = beta[i][t] # for i not equal j
    if (y[j]*sum( alpha[i][t]*K[i,j], i from 1,...,n ) < 1)
        beta[j][t+1] = beta[j][t] + y[j]
    else
        beta[j][t+1] = beta[j][t]
}
return average (alpha[i][1], ..., alpha[i][T]) # for i from 1,...,n
\end{pseudo}

\pause
Note: The vectors $W^{(t)}$ of previous algorithm are $\sum_{i=1}^n\alpha_i^{(t)}\psi(\x_i)$. But, writing $\overline{W}$ for average of the $W^{(t)}$, to get prediction on unseen $\x\in\mathbb R^d$ we just need

    \vspace{-7pt}
    {\small
    \[\langle\overline{W}, \psi(\x)\rangle = \sum_{i}\overline{\alpha}_i K(\x_i, \x).\]
    }
