\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}

%\graphicspath{{../../}}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{Feb 18, 2025}
\title{Variations on theme of Linear Regression}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Multiple variables}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
Before now, we focused on so-called \emph{simple} linear regression, where there is a single independent variable $x$ from which we predict $y$-values.

Recall the \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Advertising.csv'} data set.

\begin{itemize}
    \item Before, looked at the \ttt{Sales} as a function of \ttt{TV} (advertising budget). The data has budgets for other media: \ttt{Radio} and \ttt{Newspaper}.
    \pause
    \item Fitting \ttt{Sales} to each one with simple linear regression (one for \ttt{TV}, one for \ttt{Radio}, one for \ttt{Newspaper}) is not right. 
    \begin{itemize}
        \item Ignores that all are contributing together to \ttt{Sales}.
        \item Doesn't give predictive ability that matches data.
    \end{itemize}
\end{itemize}

\end{frame}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
    Rather than fit separate simple linear regressions, use a single model with multiple independent variables. 
\pause

    If $x_1,x_2,\ldots,x_d$ are the variables, use the model 
    \[p_0x_1 + p_1x_2 + \ldots + p_{d-1}x_d + p_d + \varepsilon\]
    where $p_i, i=0,1,\ldots,d$ are the coefficients to be fit from the data and $\varepsilon$ is a random variable with expected value 0.
    \begin{itemize}
        \item Simple linear regression case, $d=1$: $p_0$ is the slope, $p_1$ is intercept.
        \item Advertising data set: independent variables are \ttt{TV}, \ttt{Radio}, \ttt{Newspaper}
    \end{itemize}

\pause
    To find the coefficients, alter procedure a bit. Now, $A$ has column for each variable and last column of ones: $A = [\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_d, \vec{1}]$.

    Just as before, the coefficients ${\bf p} = (p_0,\ldots, p_d)$ are given by $(A^TA)^{-1}(A^T{\bf y})$.

\end{frame}

%%%%
\begin{frame}
\frametitle{Advertising example}
    Using $x_1$ for the \ttt{TV} budget, $x_2$ for \ttt{Radio}, and $x_3$ for \ttt{Newspaper}, multiple linear regression for the Advertising data set is approximately 
        \[\textsf{Sales} = 0.04576x_1 + 0.18853x_2 + -0.00104x_3 + 2.93889 + \varepsilon.\]
    
    Contrast this with what you get if you do three separate linear regressions.
    \centering
    \begin{tabular}{c c c}
        \ttt{TV} & \ttt{Radio} & \ttt{Newspaper} \\ 
        \hline 
        $0.04754x_1 + 7.03259$ & $0.2025x_2 + 9.31164$ & $0.05469x_3 + 12.3514$
    \end{tabular}

\end{frame}

%%%%
\begin{frame}
\frametitle{Confidence intervals}
How close do we suspect $\hat{m}$ and $\hat{b}$ to be to the ``true'' (population) slope and intercept?

\pause
\textbf{Standard error (SE):}  Suppose that for our error term $\varepsilon$, we have $\operatorname{Var}(\varepsilon) = \sigma^2$. Sample size: $n$.

\pause
Using $\bar{x}$ for the average of $x_1,\ldots,x_n$,

\[SE(\hat{m})^2 = \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2};\]
\[SE(\hat{b})^2 = \sigma^2\left(\frac1{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\right).\]

\pause
\emph{Roughly}, these are the amount, on average, that $\hat{m}$ (resp.\ $\hat{b}$) differs from true slope $m$ (resp.\ true intercept $b$).

\pause
$\sigma$ is unknown, but can estimate it with \textbf{residual standard error}:
    \[\hat{\sigma}^2 = RSE^2 = \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n-2}.\]

\end{frame}


%%%%
\begin{frame}
    \frametitle{Confidence intervals}
    How close do we suspect $\hat{m}$ and $\hat{b}$ to be to the ``true'' (population) slope and intercept?
    
    Formulae: 
    \[SE(\hat{m})^2 = \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2};\]
    \[SE(\hat{b})^2 = \sigma^2\left(\frac1{n} + \frac{\bar{x}^2}{\sum_{i=1}^n(x_i - \bar{x})^2}\right).\]
    
    Estimate:
        \[\sigma^2 \approx RSE^2 = \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n-2}.\]
    
        \pause
    Can get (roughly) 95\% confidence interval\footnote{95\% of the time, these intervals contain $m$, $b$.} with $\pm 2SE$: 
        \[(\hat{m} - 2SE(\hat{m}), \hat{m} + 2SE(\hat{m}))\]
    and 
        \[(\hat{b} - 2SE(\hat{b}), \hat{b} + 2SE(\hat{b})).\]
    
\end{frame}

\section{Measuring how well LSR line fits}

%%%%
\begin{frame}
\frametitle{Mean Squared Error}
How to measure how well the data fits to regression line?

\pause 
In linear regression, we found $\hat{y}_i, 1\le i\le n$ so that the points $(x_1,\hat{y}_1), \ldots, (x_n, \hat{y}_n)$ fit exactly to a line. Could use average of $(y_i - \hat{y}_i)^2$ as our measure.
    \[ \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2. \]
\pause
\begin{itemize}
    \item Called the mean squared error, MSE, of the LSR line.
    \item Larger MSE (for same sample size), the farther $y_i$ is from $\hat{y}_i$, on average.
\end{itemize}

\pause
Closely related to RSE (residual standard error). Recall, 
    \[\textsf{RSE} = \sqrt{\frac{1}{n-2}\sum_{i=1}^n(y_i - \hat{y}_i)^2}.\]
So $\textsf{MSE} = \frac{n-2}{n}\textsf{RSE}^2$.
\end{frame}


\end{document}