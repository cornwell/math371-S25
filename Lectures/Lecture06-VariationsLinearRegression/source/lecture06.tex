\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}

%\graphicspath{{../../}}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{Feb 18, 2025}
\title{Variations on theme of Linear Regression}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Multiple variables}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
Before now, we focused on \emph{simple} linear regression, with a single independent variable $x$ used to predict values of $\hat{y}$.

Recall the \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Advertising.csv'} data set.

\pause
\begin{itemize}
    \item Before, looked at the \ttt{Sales} ($y$) as a function of \ttt{TV} (advertising budget, $x$). In data set, budgets for other media: \ttt{Radio} and \ttt{Newspaper}.
    \pause
    \item Fitting \ttt{Sales} to each one with simple linear regression (one for \ttt{TV}, one for \ttt{Radio}, one for \ttt{Newspaper}) is inadequate. 
    \pause
    \begin{itemize}
        \item Ignores that all are contributing together to \ttt{Sales}.
        \item Doesn't give predictive ability that matches data.
    \end{itemize}
\end{itemize}

\end{frame}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
    Rather than fit separate simple linear regressions, use a single model with more than one independent variable {--} \textbf{multiple linear regression}. 
\pause

    If $x_0,x_1,\ldots,x_{d-1}$ are the variables, use the model 
    \[y = p_0x_0 + p_1x_1 + \ldots + p_{d-1}x_{d-1} + p_d + \varepsilon\]
    where $p_i, i=0,1,\ldots,d$ are coefficients to be fit from the data; $\varepsilon$ is random variable with expected value 0.
    \pause
    \begin{itemize}
        \item Simple linear regression case, $d=1$: $p_0$ is the slope, $p_1$ is intercept.
        \pause
        \item Advertising data set: independent variables are \ttt{TV}, \ttt{Radio}, \ttt{Newspaper}; $d = 3$.
    \end{itemize}
\vfill

\end{frame}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
    To find the coefficients, alter procedure a bit. 
    
    Matrix $A$ is size $n\times(d+1)$ and has column for each variable (and a column of ones). That is, treating each $\vec{x}_i$ as a column vector (with one entry for each data point),
        \[A = \left[\vec{x}_0, \quad\vec{x}_1, \quad\ldots, \quad\vec{x}_{d-1}, \quad\vec{1}\right].\]

    \pause
    Just as before, the coefficients ${\bf p} = (\hat{p}_0,\ldots, \hat{p}_d)$ are given by $(A^TA)^{-1}(A^T{\bf y})$, provided that $A^TA$ is invertible. 

    \pause
    The matrix $A^TA$ is invertible if $A$ has rank $d+1$ (when $\{\vec{x}_0, \vec{x}_1, \ldots, \vec{x}_{d-1}, \vec{1}\}$ a linearly indpt.\ set). \footnote{If a ``real world'' data set with $n\ge d+1$, almost surely.} 
    \pause
    \begin{itemize}
        \item Larger $d$ $\to$ more likely $A^TA$ is poorly conditioned (potential issues from numerically computing its inverse).
    \end{itemize}

\end{frame}

%%%%
\begin{frame}
    \frametitle{Advertising example}
    $x_0$ for \ttt{TV} budget; \quad $x_1$ for \ttt{Radio} budget; \quad $x_2$ for \ttt{Newspaper} budget.
    
    \pause
    Writing $y$ for \ttt{Sales}, multiple linear regression model for Advertising data is approximately 
        \[\hat{y} = 0.0458x_0 + 0.1885x_1 - 0.001x_2 + 2.9389.\]
    \pause
    Interpretation: given fixed budget for radio and newspaper ads, increasing TV ad budget by \$1000 will increase sales by around 46 units (in each market, on average).

    \pause
    Contrast with result of three separate linear regressions, below.
    
    \begin{center}
        \begin{tabular}{c ||c|c|c}
            Variable & \ttt{TV} & \ttt{Radio} & \ttt{Newspaper} \\ 
            \hline 
            LSR line    &  {\footnotesize$0.0475x_0 + 7.0326$} & {\footnotesize$0.2025x_1 + 9.3116$} & {\footnotesize$0.0547x_2 + 12.3514$} \\
            \hline 
            $R^2$       &  {\footnotesize$0.612$}  &  {\footnotesize$0.332$}  &  {\footnotesize$0.052$}
        \end{tabular}
    \end{center}
\end{frame}

%%%%
\begin{frame}
\frametitle{$R^2$}
Previously: $R^2$ for predicting \ttt{Sales} from \ttt{TV} significantly higher than from either \ttt{Radio} or \ttt{Newspaper}. 

\pause
Can get $R^2$ from regression model, either using 2 of the variables, or using all 3. 

First, recall result from simple regression:
\begin{center}
    \begin{tabular}{l c c c}
        Independent var. & \ttt{TV} & \ttt{Radio} & \ttt{Newspaper} \\ 
        \hline 
        $R^2$       &  0.612  &  0.332  &  0.052
    \end{tabular}
\end{center}

\pause
Now, $R^2$ for all possible pairs of two:
\begin{center}
    \begin{tabular}{l c c c}
        Two vars. & \ttt{TV}, \ttt{Radio} & \ttt{TV}, \ttt{Newspaper} & \ttt{Radio}, \ttt{Newspaper} \\ 
        \hline 
        $R^2$       &  0.89719  &  0.646  &  0.333
    \end{tabular}
\end{center}

\pause
The value of $R^2$ with all three predictor (independent) variables is: 0.89721. {\color{strings}What conclusion can we draw?}

\end{frame}

%%%%
\begin{frame}
    \frametitle{How small to decide not significant?}
    
    \pause 
    Hypothesis testing: choose a $p$-value threshold (often $<0.05$ or $<0.01$). The $p$-value corresponds to some $t$-statistic {--} use regression coefficient ($\hat{p}_i$ for $x_i$) and standard error.
    \begin{itemize}
        \item In example, if using simple linear regression on \ttt{Newspaper}, would get the variable is significant. However, using multiple regression with \ttt{TV}, \ttt{Radio}, and \ttt{Newspaper}, get very large $p$-value $\to$ so, not significant.
    \end{itemize}

    \pause 
    Another perspective: $p$ is large when $t$-statistic is small, which is when $SE$ is large \emph{relative to size of }$\hat{p}_i$. $SE$ is roughly how far $\hat{p}_i$ is from population coeff.\ $p_i$, on average.

    \pause 
    Can take (many) random subsamples of your data (fraction of whole data set); compute $\hat{p}_i$ for those. Find standard deviation of them $\to$ approximates $SE$. \newline 
    Next, approximate $p_i$ with regression coefficient from your whole data set. If the standard deviation above, divided by this coeff., is (order(s) of magnitude) larger than the same thing for other variables, this variable is not significant.
\end{frame}

\section{Polynomial fitting}

%%%%
\begin{frame}
    \frametitle{Using powers ``like'' other variables}
    Often, a linear model does not seem like a good fit for our data. What about trying to fit the data to a polynomial?

    \pause
    i.e., consider the model 
        \[p_0x^d + p_1x^{d-1} + \ldots + p_{d-1}x + p_d + \varepsilon\]
    for some degree $d$, and find the coefficients which give best fit polynomial.
    
    \pause
    For the procedure, use essentially the same idea for the matrix $A$, but using powers of your variable $x$ (or, variables) instead of using different independent variables. Given data with $x$-coordinates $x_1,x_2,\ldots,x_m$, the matrix $A$ is known as a \textbf{Vandermonde matrix}.
    
    \vfill
\end{frame}

%%%%
\begin{frame}
\frametitle{Example}
Taking the \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'College.csv'} data set from the DataSets folder. Two of the columns are \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Top10perc'} and \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Top25perc'}. For the schools in the data set, these columns give the percentage of the entering class that were in the top 10\% (resp.\ 25\%) of their graduating high school class.\footnote{Removed rows that contained schools receiving fewer than 2500 applications.}
    

\pause
Here is the data set with a least squares line. The value of $R^2$ is 0.791. 
\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg1.png}
\end{center}

\end{frame}

%%%%
\begin{frame}
    \frametitle{Example}
    Here is the data set with a least squares line. The value of $R^2$ is 0.791. 
\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg1.png}
\end{center}

    \pause 
    Next, the data set with a least squares quadratic polynomial fit. The $R^2$ value is 0.854.

\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg2.png}
\end{center}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Value of $R^2$ as polynomial degree increases}
    What will happen to the value of $R^2$ if we increase the degree of the polynomial that we fit to the data?

    \pause
    \begin{itemize}
        \item Note: Suppose that $n>d$. A Vandermonde matrix for $x$-values $x_1,x_2,\ldots, x_n$, which has $d+1$ columns (so, highest power is $x_i^d$), will have rank $d+1$ if and only if there are $d+1$ of the $x_i$ that are distinct.
        \pause
        \begin{itemize}
            \item[] \textit{If $x_1,x_2,\ldots,x_{d+1}$ are pairwise distinct, say, then the determinant of the $(d+1)\times(d+1)$ submatrix for their corresponding rows is}
            \[{\displaystyle\prod_{1\le i<j\le d+1}(x_j - x_i)}.\]
        \end{itemize}
    \end{itemize}

    \pause
    $A_0$: the Vandermonde matrix used in procedure to fit a polynomial of degree $d$; set $A_1$ to be one used for polynomial of degree $d+1$. \footnote{So, $A_{1}$ has all the columns of $A_0$, and one additional column.} From Note, as long as enough of the $x_i$ are distinct, $\operatorname{rank}(A_1) = \operatorname{rank}(A_0)+1$.

    \pause
    This means: $\textsf{Col}(A_0)$ is proper subspace of $\textsf{Col}(A_1)$. So, using it makes $|y - \hat{y}|^2$ smaller. Since $\sum(y - \bar{y})^2$ is unchanged, makes $R^2$ closer to 1.
\end{frame}

\end{document}